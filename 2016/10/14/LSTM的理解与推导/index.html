<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>LSTM学习笔记 | Jiaxun&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Long Short-Term Memory（LSTM） 是一种循环神经网络（Recurrent Neural Network, RNN）。跟所有RNN一样，在网络单元足够多的条件下，LSTM可以计算传统计算机所能计算的任何东西。

Like most RNNs, an LSTM network is universal in the sense that given enough network">
<meta property="og:type" content="article">
<meta property="og:title" content="LSTM学习笔记">
<meta property="og:url" content="https://jiaxuncai.github.io/2016/10/14/LSTM的理解与推导/index.html">
<meta property="og:site_name" content="Jiaxun's Blog">
<meta property="og:description" content="Long Short-Term Memory（LSTM） 是一种循环神经网络（Recurrent Neural Network, RNN）。跟所有RNN一样，在网络单元足够多的条件下，LSTM可以计算传统计算机所能计算的任何东西。

Like most RNNs, an LSTM network is universal in the sense that given enough network">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006y8lVajw1f8s2b8wafyj30qw0owjtm.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/006y8lVagw1f8s2co0uixj30su0m2mzn.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/006y8lVagw1f8s2cog4sij30tu0zu0v8.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006y8lVagw1f8s2couv7gj30cq018jrc.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/006y8lVagw1f8st5b12muj307i01k3yd.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006y8lVagw1f8st5lzm59j30ac02gmx2.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/006y8lVajw1f8t0c4cv4lj30v405mq3h.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/006y8lVajw1f8st6pgw9uj30vw050dgg.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/006y8lVagw1f8st60foo7j30m202y0sz.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/006y8lVajw1f8stb3pvwmj30i602g3yj.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/006y8lVajw1f8stbdlafwj30hq02st8q.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006y8lVajw1f8spmw90ibj30ye0fk0vv.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006y8lVajw1f8squqpfgsj31kw0lddky.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/006y8lVajw1f8sq92lnsej30qe04x3yy.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/006y8lVajw1f8sq9wzquyj305i06qa9z.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/006y8lVagw1f8sqirs9vnj31eq0fo402.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/006y8lVajw1f8sqwzrkx0j31eq0fognr.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006y8lVagw1f8sr6rec6oj31eq0foabt.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/006y8lVajw1f8sr9ct7p8j31eq0fotat.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/006y8lVajw1f8t8il3l76j30ms04y3yp.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/006y8lVajw1f8t8ijefsxj30dw04ct8r.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006y8lVajw1f8t8ij8d0zj30c80480ss.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/006y8lVajw1f8t8iiwi94j30v6032glx.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/006y8lVajw1f8t8ilr7dqj30yg04q3yz.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/006y8lVajw1f8t8ilier7j315g084jsf.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/006y8lVajw1f8t8ikolmrj30hg02mt8q.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006y8lVajw1f8spmw90ibj30ye0fk0vv.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/006y8lVajw1f8t8ijx4crj311s08smxx.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/006y8lVajw1f8t8ikd0btj30h004ot8y.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/006y8lVajw1f8u9djvlr2j313405w3za.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/006y8lVagw1f8u9ke7x5wj30j004wmxe.jpg">
<meta property="og:image" content="http://ww2.sinaimg.cn/large/801b780agw1f8u9nl7zoij213o074my1.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/801b780agw1f8u9pxpt6wj20ga04w0sv.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/801b780agw1f8u9qcccctj20lu02gdfy.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/801b780agw1f8u9qlkzpoj213k02aq3c.jpg">
<meta property="og:updated_time" content="2016-10-16T10:14:06.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LSTM学习笔记">
<meta name="twitter:description" content="Long Short-Term Memory（LSTM） 是一种循环神经网络（Recurrent Neural Network, RNN）。跟所有RNN一样，在网络单元足够多的条件下，LSTM可以计算传统计算机所能计算的任何东西。

Like most RNNs, an LSTM network is universal in the sense that given enough network">
<meta name="twitter:image" content="http://ww1.sinaimg.cn/large/006y8lVajw1f8s2b8wafyj30qw0owjtm.jpg">
  
    <link rel="alternative" href="/atom.xml" title="Jiaxun&#39;s Blog" type="application/atom+xml">
  
  
  <script src="/style.js"></script>
  

</head>

<body>
  <div id="container">
    <div class="left-col">
      <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img src="http://ww4.sinaimg.cn/large/006y8lVagw1f8s3abm2orj31hc1hctga.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Jiaxun Cai</a></h1>
		</hgroup>

		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
	        
    		
    			
    			<a class="js-smart-menu" data-idx="0" href="javascript:void(0)">所有文章</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="1" href="javascript:void(0)">标签</a>
    			
    			
            
    			
            
    			
    			<a class="js-smart-menu" data-idx="2" href="javascript:void(0)">关于我</a>
    			
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="http://github.com/JiaxunCai" title="github">github</a>
		        
					<a class="mail" target="_blank" href="mailto:jiaxuncai@sjtu.edu.cn" title="mail">mail</a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"><i class="icon-list"></i></div>
  		<h1 class="header-author js-mobile-header hide">Jiaxun Cai</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				
					<img src="http://ww4.sinaimg.cn/large/006y8lVagw1f8s3abm2orj31hc1hctga.jpg" class="js-avatar">
				
			</div>
			<hgroup>
			  <h1 class="header-author">Jiaxun Cai</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="http://github.com/JiaxunCai" title="github">github</a>
			        
						<a class="mail" target="_blank" href="mailto:jiaxuncai@sjtu.edu.cn" title="mail">mail</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
        <article id="post-LSTM的理解与推导" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      LSTM学习笔记
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Long Short-Term Memory（LSTM） 是一种循环神经网络（Recurrent Neural Network, RNN）。跟所有RNN一样，在网络单元足够多的条件下，LSTM可以计算传统计算机所能计算的任何东西。</p>
<blockquote>
<p>Like most RNNs, an LSTM network is universal in the sense that given enough network units it can compute anything a conventional computer can compute. <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank" rel="external">维基百科</a></p>
</blockquote>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>传统前馈神经网络（feedforward neural networks）如下图所示：<br><a id="more"></a><br><img src="http://ww1.sinaimg.cn/large/006y8lVajw1f8s2b8wafyj30qw0owjtm.jpg" width="600"></p>
<p>前馈神经网络只从输入节点接受信息，它只能对输入空间进行操作，对不同时间序列的输入没有“记忆”。在前馈神经网络中，信息只能从输入层流向隐藏层，再流向输出层。这种网络无法解决带有时序性的问题，比如预测句子中的下一个单词，这种情况下，往往需要使用到前面已知的单词。假设要预测这样一句话：百度是一家<em>__</em>公司。显然，受到前面的词语“百度”的影响，横线中填入“互联网”的概率远大于“金融”，即使单从语法上考虑，填入“金融”也是正确的。</p>
<p>RNN与前馈神经网络最大的不同是，它不仅能对输入空间进行操作，还能对内部状态空间进行操作，它的结构如下：<br><img src="http://ww4.sinaimg.cn/large/006y8lVagw1f8s2co0uixj30su0m2mzn.jpg" width="600"></p>
<p>可以看到，RNN的隐藏层多了一条连向自己的边。因此，它的输入不仅包括输入层的数据，还包括了来自上一时刻的隐藏层的输出。</p>
<p>RNN可以采用BPTT（Back-Propagation Through Time）算法进行学习。BPTT和BP算法类似，都是基于梯度的训练方法。首先，将RNN按时间序列展开，如下图：<br><img src="http://ww4.sinaimg.cn/large/006y8lVagw1f8s2cog4sij30tu0zu0v8.jpg" width="600"></p>
<p>图中表示的是时间步长数为3的RNN的展开。一般的，时间步长为T的RNN展开后将含有T个隐藏层，T可以是任意的。当T为1时，RNN退化为一个普通的前馈神经网络。将RNN展开之后，就可以使用与训练带BP的前馈神经网络类似的方法进行训练。有一点需要注意的是，在展开的RNN中，每一层隐藏层实际上是相同的（只是在不同时刻的副本），也就是说，它们最后得到的参数必须是一致的。在训练过程中，不同时刻的隐藏层的参数可能会不一致，最后可以将它们的平均数作为模型的参数。</p>
<p>关于RNN的内容，具体的可以参考“<a href="http://axon.cs.byu.edu/~martinez/classes/678/Papers/RNN_Intro.pdf" target="_blank" rel="external">A guide to recurrent neural networks and backpropagation</a>”。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h3 id="梯度消失与避免"><a href="#梯度消失与避免" class="headerlink" title="梯度消失与避免"></a>梯度消失与避免</h3><p>在实际应用中，上述RNN模型存在着梯度消失和梯度爆炸的问题。根据链式法则，输出误差对于输入层的偏导等于各层偏导的乘积（关于前馈神经网络的误差传播参考“<a href="http://www.cnblogs.com/charlotte77/p/5629865.html" target="_blank" rel="external">一文弄懂神经网络中的反向传播法——BackPropagation</a>“）。<br>假设使用的是平均平方误差，则在时刻t，输出层k的误差信号表示为（以下推导来自“<a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="external">Long Short-Term Memory</a>”，本文采用与论文一致的表示方法）：<br><img src="http://ww1.sinaimg.cn/large/006y8lVagw1f8s2couv7gj30cq018jrc.jpg" width="300"></p>
<p>其中,<br><img src="http://ww4.sinaimg.cn/large/006y8lVagw1f8st5b12muj307i01k3yd.jpg" width="200"><br>是表示非输入单元，f<sub>i</sub>是可微函数，</p>
<img src="http://ww1.sinaimg.cn/large/006y8lVagw1f8st5lzm59j30ac02gmx2.jpg" width="250">
<p>表示当前网络单元的输入，w<sub>ij</sub>是单元j和i的之间权重。非输出单元j的反向误差信号为：<br><img src="http://ww3.sinaimg.cn/large/006y8lVajw1f8t0c4cv4lj30v405mq3h.jpg" width="300"></p>
<p>对于时间步长为q的RNN网络，在t-q时刻的的误差可以通过以下递归函数来求解：<br><img src="http://ww2.sinaimg.cn/large/006y8lVajw1f8st6pgw9uj30vw050dgg.jpg" width="600"></p>
<p>令l<sub>q</sub> = v，l<sub>0</sub> = u，上式可以进一步写成：<br><img src="http://ww4.sinaimg.cn/large/006y8lVagw1f8st60foo7j30m202y0sz.jpg" width="500"></p>
<p>由于梯度最终以乘积的形式得出，若乘式中的每一项（或大部分）都大于1，<br><img src="http://ww4.sinaimg.cn/large/006y8lVajw1f8stb3pvwmj30i602g3yj.jpg" width="300"></p>
<p>则将导致梯度爆炸；若每一项都小于1，<br><img src="http://ww2.sinaimg.cn/large/006y8lVajw1f8stbdlafwj30hq02st8q.jpg" width="300"></p>
<p>则随着乘法次数的增加，梯度会消失。<br>梯度消失和梯度爆炸都会严重影响学习的过程。</p>
<p>为了避免梯度消失和梯度爆炸，一个简单的做法是强制让流过每个神经元的误差都为1，即</p>
<p>简单推导可以知道，f是一个线性函数。这样就保证了误差将以参数的形式在网络中流动，不会出现梯度爆炸或者梯度消失的问题，把这样的结构称为CEC（constant error carousel）。但是这种做法存在着权重冲突的问题。</p>
<h3 id="权重冲突问题与解决"><a href="#权重冲突问题与解决" class="headerlink" title="权重冲突问题与解决"></a>权重冲突问题与解决</h3><p>前面说到，RNN的隐藏层同时接受外界信息和上一时刻隐藏层的输出作为输入，回到前面将RNN展开成深层网络的情况。隐藏层在每一个时刻t的输出都通过权重向量U影响着下一个时刻t+1的隐藏层。而实际上，对于不同的时刻t, t+1, …, t+k，图中隐藏层的权重向量V和U是相同的。虽然在展开图中它们看起来像是不同层次的节点，但实际上，它们都是同一个实体。</p>
<p>这就产生一个问题，在某一个时刻t，隐藏层可能需要使得权重向量U整体有一个较大的值，即t-1时刻隐藏层的输出对t时刻很重要；而在时刻t+k，隐藏层可能需要使权重U有一个较小的值，也就是说此时隐藏层不想受到t+k-1时刻的计算结果的影响。考虑现实的例子，假如要预测以下句子中动词的形式，括号中为需要预测的词：I may (go) home and (get) my book, but that (depends)。句子中go和get的形式受到may的影响，而depends的形态由that决定，而与may无关。因此预测过程中，当读入and时，may的影响还在，因此应该增大权重U的值，而在读入that的时候，应该同时减小来自上一时刻的隐藏层输出的影响，即减小权重U的，这就与前面产生了冲突。</p>
<p>为了解决这种冲突，Sepp Hochreiter和Jürgen Schmidhuber早在1997年就在论文“<a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="external">Long Short-Term Memory</a>”提出了LSTM，下面的部分图以及公式来自该论文。为了使隐藏层的输出对下一个时刻的影响变得可控，LSTM引入了输入门，输出门的概念。LSTM的基本单元称为记忆元件（memory cell），它是在CEC的基础上扩展而成的，如下图：<br><img src="http://ww1.sinaimg.cn/large/006y8lVajw1f8spmw90ibj30ye0fk0vv.jpg" width="600"></p>
<p>图中，3表示输入门（input gate）；6表示输出门（output gate）；4是CEC单元，可以看到它有一条到自己的权值为1的边。之后，又有人将LSTM进一步扩展，引入了遗忘门，如下图（图片内容出自“<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a>”）：<br><img src="http://ww1.sinaimg.cn/large/006y8lVajw1f8squqpfgsj31kw0lddky.jpg" alt="LSTM-chain"></p>
<p>为了方便表示，把上一个图称为lstm-1，这个图称为lstm-2。lstm-2中与lstm-1编号相同的单元分别一一对应，可以看到，lstm-2比lstm-1多了单元8，它就是所谓的遗忘门。lstm-2中元素的含义如下：<br><img src="http://ww2.sinaimg.cn/large/006y8lVajw1f8sq92lnsej30qe04x3yy.jpg" width="600"></p>
<img src="http://ww2.sinaimg.cn/large/006y8lVajw1f8sq9wzquyj305i06qa9z.jpg" width="100"> 表示一个门，它由一个网络层和一个乘法单元构成。<br><br>### LSTM中的信息流传递<br><br>（以下图片内容出自“<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a>”）<br>首先，记忆元件（memory cell）接受上一个时刻的输出（h<sub>t-1</sub>）以及这个时刻的外界信息（x<sub>t</sub>）作为输入，将它们合并成一个长向量，经过𝞼变换成为f<sub>t</sub>。<br><img src="http://ww3.sinaimg.cn/large/006y8lVagw1f8sqirs9vnj31eq0fo402.jpg" alt=""><br><br>接着，前面说到的两个输入合并成的向量又分别进行了𝞼变换和tanh变换，成为i<sub>t</sub>和Ĉ<sub>t</sub>进入输入门。其中i<sub>t</sub>的值用于决定是否要接受输入信息（即Ĉ<sub>t</sub>）。<br><img src="http://ww2.sinaimg.cn/large/006y8lVajw1f8sqwzrkx0j31eq0fognr.jpg" alt=""><br><br>之后，根据f<sub>t</sub>的值决定是否保留上一时刻隐藏层CEC的输出（C<sub>t-1</sub>），再将经过i<sub>t</sub>缩放之后的输入Ĉ<sub>t</sub>累加到CEC（C<sub>t-1</sub>）中，成为C<sub>t</sub>。<br><img src="http://ww1.sinaimg.cn/large/006y8lVagw1f8sr6rec6oj31eq0foabt.jpg" alt=""><br><br>最后在输出门中，由O<sub>t</sub>决定是否将经过tanh变换的C<sub>t</sub>输出，作为下一时刻输入的一部分。<br><img src="http://ww4.sinaimg.cn/large/006y8lVajw1f8sr9ct7p8j31eq0fotat.jpg" alt=""><br><br>Sepp Hochreiter和Jürgen Schmidhuber的论文中还提到一种称为记忆元件块（memory cell block）的部件。由S个共享相同的输入门、输出门（论文中的LSTM并没有记忆门）的记忆元件所构成的结构称为“大小为S的记忆元件块”。当S = 1时，该结构就是普通的记忆元件。<br><br>### LSTM的误差传播<br><br>首先，规定下标的表示如下：<br><br>- k：输出单元<br>- i：隐藏单元<br>- C<sub>j</sub>：第j个记忆元件块<br>- C<sub>j</sub><sup>v</sup>：第j个记忆元件块C<sub>j</sub>中的第v个单元<br>- l, m, u：任意的网络单元<br>- t：给定输入序列所有的时间步长<br><br>在t时刻，LSTM的平方误差计算如下:<br><img src="http://ww2.sinaimg.cn/large/006y8lVajw1f8t8il3l76j30ms04y3yp.jpg" width="400">
<p>其中，t<sup>k</sup>(t)是输出单元k在t时刻的输出目标。<br>在学习率为𝞪的条件下，w<sub>lm</sub>基于梯度的更新如下：<br><img src="http://ww3.sinaimg.cn/large/006y8lVajw1f8t8ijefsxj30dw04ct8r.jpg" width="200"></p>
<p>将单元l在t时刻的误差（error）定义为：<br><img src="http://ww1.sinaimg.cn/large/006y8lVajw1f8t8ij8d0zj30c80480ss.jpg" width="170"></p>
<p>使用标准的方向误差传播算法（backdrop）就可以计算出输出单元（output unit，l = k）、隐藏单元（hidden unit，l = i）、输出门单元（output gate unit，l = out<sub>j</sub>）的权重更新：<br><img src="http://ww2.sinaimg.cn/large/006y8lVajw1f8t8iiwi94j30v6032glx.jpg" width="500"></p>
<img src="http://ww3.sinaimg.cn/large/006y8lVajw1f8t8ilr7dqj30yg04q3yz.jpg" width="550">
<img src="http://ww3.sinaimg.cn/large/006y8lVajw1f8t8ilier7j315g084jsf.jpg" width="600">
<p>对于所有可能的单元l，时刻t对权重w<sub>lm</sub>所贡献的更新为：<br><img src="http://ww3.sinaimg.cn/large/006y8lVajw1f8t8ikolmrj30hg02mt8q.jpg" width="300"></p>
<p>e<sub>out<sub>j</sub></sub>(t)的计算式子中，括号内的h函数一开始把我卡住了，但是看回记忆元件的结构图：<br><img src="http://ww1.sinaimg.cn/large/006y8lVajw1f8spmw90ibj30ye0fk0vv.jpg" width="600"></p>
<p>可以发现输出门的计算公式为：gate<sub>out</sub> = h*y<sup>out<sub>j</sub></sup>，其中y<sup>out<sub>j</sub></sup>是net<sub>out<sub>j</sub></sub>的函数，h与net<sub>out<sub>j</sub></sub>无关，根据求导法则，h被保留了下来，gate<sub>out</sub>‘ = h*(y<sup>out<sub>j</sub></sup>)’，于是得到上式。</p>
<p>剩下的对输入门单元（l = in<sub>j</sub>）和记忆元件单元<l =="" c<sub="">j<sup>v</sup>的更新与常规的单元会有些差别。定义内部状态S<sub>C<sub>j</sub><sup>v</sup></sub>的误差为：<br><img src="http://ww3.sinaimg.cn/large/006y8lVajw1f8t8ijx4crj311s08smxx.jpg" width="550"></l></p>
<p>虽然上式乍看之下形式有些复杂，但仔细分析可以发现，这与求输出门的梯度的情况是类似的，由于y<sup>out<sub>j</sub></sup>是与S<sub>C<sub>j</sub></sub>无关的项，所以在求导过程中被保留了下来。</p>
<p>由以上推导可以得到，当l = in<sub>j</sub>或者l = C<sub>j</sub><sup>v</sup>，v = 1, …, S<sub>j</sub>时的误差：<br><img src="http://ww2.sinaimg.cn/large/006y8lVajw1f8t8ikd0btj30h004ot8y.jpg" width="200"></p>
<p>中间状态单元S<sub>C<sub>j</sub><sup>v</sup></sub>对于输入net<sub>in<sub>j</sub></sub>单元的权重w<sub>in<sub>j</sub></sub>的偏导可以计算如下：<br><img src="http://ww2.sinaimg.cn/large/006y8lVajw1f8u9djvlr2j313405w3za.jpg" width="600"></p>
<p>因此，时刻t对w<sub>in<sub>j</sub>m</sub>更新的贡献为：<br><img src="http://ww1.sinaimg.cn/large/006y8lVagw1f8u9ke7x5wj30j004wmxe.jpg" width="270"></p>
<p>类似地，可以得到S<sub>C<sub>j</sub><sup>v</sup></sub>对于net<sub>C<sub>j</sub><sup>v</sup></sub>的权重w<sub>C<sub>j</sub><sup>v</sup></sub>的偏导为：<br><img src="http://ww2.sinaimg.cn/large/801b780agw1f8u9nl7zoij213o074my1.jpg" width="600"></p>
<p>因此，时刻t对w<sub>C<sub>j</sub><sup>v</sup></sub>更新的贡献为：<br><img src="http://ww4.sinaimg.cn/large/801b780agw1f8u9pxpt6wj20ga04w0sv.jpg" width="270"></p>
<p>以上就是反向传播算法所需要使用到的等式。在更新权重的过程中，每个权重总的更新值是所有时刻t对w权重更新的贡献之和。</p>
<h3 id="LSTM参数更新的计算复杂度"><a href="#LSTM参数更新的计算复杂度" class="headerlink" title="LSTM参数更新的计算复杂度"></a>LSTM参数更新的计算复杂度</h3><p>LSTM每次更新的计算复杂度为：<br> <img src="http://ww1.sinaimg.cn/large/801b780agw1f8u9qcccctj20lu02gdfy.jpg" width="300"></p>
<p>其中，K表示输出单元的数量，C表示记忆元件块的数量，S&gt;0表示记忆元件块的大小，H是隐藏单元的数量，I是与记忆元件、门单元和隐藏单元直接相连（forward-connected）的单元的数量，而</p>
<img src="http://ww3.sinaimg.cn/large/801b780agw1f8u9qlkzpoj213k02aq3c.jpg" width="600">
<p>是权重的数量。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/10/14/LSTM的理解与推导/" class="archive-article-date">
  	<time datetime="2016-10-14T09:16:21.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-10-14</time>
</a>
      
      

      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
  
    <a href="/2016/10/05/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Hello World</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>




<div class="share_jia">
	<!-- JiaThis Button BEGIN -->
	<div class="jiathis_style">
		<span class="jiathis_txt">Share to: &nbsp; </span>
		<a class="jiathis_button_facebook"></a> 
	    <a class="jiathis_button_twitter"></a>
	    <a class="jiathis_button_plus"></a> 
	    <a class="jiathis_button_tsina"></a>
		<a class="jiathis_button_cqq"></a>
		<a class="jiathis_button_douban"></a>
		<a class="jiathis_button_weixin"></a>
		<a class="jiathis_button_tumblr"></a>
    <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	</div>
	<script type="text/javascript" src="//v3.jiathis.com/code/jia.js?uid=1405949716054953" charset="utf-8"></script>
	<!-- JiaThis Button END -->
</div>









      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2016 Jiaxun Cai
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: false,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		root: "/",
		innerArchive: true
	}
</script>

<script src="/./main.js"></script>


    
<div class="tools-col">
  <ul class="btn-wrap">
    
      <li class="chose" data-hook="tools-section-all"><span class="text">全部</span><i class="icon-book"></i></li>
    
    
      <li data-hook="tools-section-tag"><span class="text">标签</span><i class="icon-price-tags"></i></li>
    
    
    
      <li data-hook="tools-section-me"><span class="text">我</span><i class="icon-smile"></i></li>
    
  </ul>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all chose">
    	</section>
    

    
    	<section class="tools-section tools-section-tag">
    			<div class="widget tagcloud" id="js-tagcloud">
    				
    			</div>
    	</section>
    

    

    
    	<section class="tools-section tools-section-me">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">&lt;br&gt; &amp;nbsp; 蔡家勋&lt;br&gt;&lt;br&gt; &amp;nbsp; 计算机科学与工程系&lt;br&gt; &amp;nbsp; 上海交通大学&lt;br&gt;&lt;br&gt;</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>